{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/manish/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 320,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "from nltk.probability import FreqDist, LidstoneProbDist\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "from autocorrect import Speller\n",
    "\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â€œ18 months ago, we expelled a boy at Nations for selling drugs in six schools.\n"
     ]
    }
   ],
   "source": [
    "# Sample data loading from a text file and remove line number\n",
    "with open('./eng_news_2020_10K-sentences-1.txt', 'r', encoding='utf-8') as file:\n",
    "    data = file.readlines()\n",
    "\n",
    "processed_data = []\n",
    "\n",
    "for line in data:\n",
    "    sentances = sent_tokenize(line)\n",
    "    for sentence in sentances:\n",
    "        words = sentence.split()[1:]\n",
    "        sentence = ' '.join(words)\n",
    "        processed_data.append(sentence)\n",
    "\n",
    "print(processed_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Model:\n",
    "    def __init__(self, ngram:int, th:int = 0.060, seed:int = 42) -> None:\n",
    "        self.model          =   {}\n",
    "        self.n              =   ngram\n",
    "        self.spell          =   Speller(lang='en')\n",
    "        self.stop_words     =   set(stopwords.words('english'))\n",
    "        self.max_next       =   100\n",
    "        self.dictionary     =   {}\n",
    "        self.thresold       =   th\n",
    "        self.seeds          =   seed\n",
    "        self.random         =   random\n",
    "        self.start_tag      =   '<s>'\n",
    "        self.stop_tag       =   '</s>'\n",
    "\n",
    "        self.random.seed(self.seeds)\n",
    "\n",
    "    def nextWord(self, context:str, random:bool = True)->str:\n",
    "        context = self.getTokens(context)[:-1]\n",
    "        return self._nextWord(tokens=context,random=random)\n",
    "    \n",
    "    def _nextWord(self, tokens:list, random=True)->str:\n",
    "        words = self._getTopWords(tokens)\n",
    "        if len(words) > 1 and random:\n",
    "            return self.random.choice(words)\n",
    "        elif len(words) > 0:\n",
    "            return words[0]\n",
    "        else:\n",
    "            return None\n",
    "        \n",
    "    def _getTopWords(self,tokens:list, n:int= None)->list:\n",
    "        req = self.n-1\n",
    "        if len(tokens) > req:\n",
    "            tokens = tokens[-req:]\n",
    "        try:\n",
    "            next_words = sorted(self.model[tuple(tokens)].items(), key= lambda x: x[1], reverse=True)\n",
    "            words = [word[0] for word in next_words if word[1] > self.thresold]\n",
    "            print(\"words: \",words)\n",
    "            if n is not None:\n",
    "                return words[:n]\n",
    "            return words\n",
    "        except:\n",
    "            return [self.stop_tag]\n",
    "        \n",
    "    def getTokens(self, sentence:str)->list:\n",
    "        tokens = []\n",
    "        token_list = list(word_tokenize(sentence.lower()))\n",
    "        token_list = [word for word in token_list if word.isalnum()]\n",
    "        token_list = list(filter(lambda token: nltk.tokenize.punkt.PunktToken(token).is_non_punct, token_list))\n",
    "        prefix = [self.start_tag]*(self.n-1)\n",
    "        sufix = [self.stop_tag]\n",
    "        tokens.extend(prefix)\n",
    "        tokens.extend(token_list)\n",
    "        tokens.extend(sufix)\n",
    "        return tokens\n",
    "    \n",
    "    def addToDictionary(self, word:str):\n",
    "        entry = self.dictionary.get(word)\n",
    "        if entry is None:\n",
    "            self.dictionary[word] = 0\n",
    "        self.dictionary[word] += 1\n",
    "        \n",
    "    def fit(self, train_data:list):\n",
    "        for sentence in train_data:\n",
    "            words = self.getTokens(sentence)\n",
    "            n_grams_list = list(ngrams(words, self.n))\n",
    "            for n_grams in n_grams_list:\n",
    "                key, value = n_grams[:-1], n_grams[-1]\n",
    "                \n",
    "                if key not in self.model:\n",
    "                    self.model[key] = {}\n",
    "                if value not in self.model[key]:\n",
    "                    self.model[key][value] = 1\n",
    "                else:\n",
    "                    self.model[key][value] += 1\n",
    "        for key in self.model:\n",
    "            total_count = float(sum(self.model[key].values()))\n",
    "            for w3 in self.model[key]:\n",
    "                self.model[key][w3] /= total_count\n",
    "\n",
    "    def complete_sentence(self, context:str, random:bool = True)->str:\n",
    "        words = self.getTokens(context)[:-1]\n",
    "        next_word = words[-1]\n",
    "        max = self.max_next\n",
    "        while  next_word !=  self.stop_tag:\n",
    "            next_word = self._nextWord(words,random)\n",
    "            words.append(next_word)\n",
    "            max -= 1\n",
    "            if max==0:\n",
    "                break\n",
    "        sentence = \" \".join(words)\n",
    "        sentence = sentence.replace(self.start_tag, \"\")\n",
    "        sentence = sentence.replace(self.stop_tag, \".\")\n",
    "        sentence = sentence.strip()\n",
    "        return sentence\n",
    "\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "words:  ['not', 'going']\n",
      "not\n"
     ]
    }
   ],
   "source": [
    "model = Model(3)\n",
    "model.fit(processed_data)\n",
    "\n",
    "print(model.nextWord(\"I am\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "words:  ['not', 'going']\n",
      "not\n",
      "words:  ['not', 'going']\n",
      "going\n",
      "words:  ['not', 'going']\n",
      "not\n"
     ]
    }
   ],
   "source": [
    "print(model.nextWord(\"I am\"))\n",
    "print(model.nextWord(\"I am\"))\n",
    "print(model.nextWord(\"I am\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "words:  ['have', 'm', 'think']\n",
      "words:  ['also', 'to', 'no', 'a', 'been']\n",
      "words:  ['be']\n",
      "words:  ['a']\n",
      "words:  []\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "('a', None)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[336], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m sentence \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcomplete_sentence\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mI\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(sentence)\n",
      "Cell \u001b[0;32mIn[333], line 80\u001b[0m, in \u001b[0;36mModel.complete_sentence\u001b[0;34m(self, context, random)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28mmax\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_next\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m  next_word \u001b[38;5;241m!=\u001b[39m  \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m</s>\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 80\u001b[0m     next_word \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nextWord\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwords\u001b[49m\u001b[43m,\u001b[49m\u001b[43mrandom\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     81\u001b[0m     words\u001b[38;5;241m.\u001b[39mappend(next_word)\n\u001b[1;32m     82\u001b[0m     \u001b[38;5;28mmax\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "Cell \u001b[0;32mIn[333], line 20\u001b[0m, in \u001b[0;36mModel._nextWord\u001b[0;34m(self, tokens, random)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_nextWord\u001b[39m(\u001b[38;5;28mself\u001b[39m, tokens:\u001b[38;5;28mlist\u001b[39m, random\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m\u001b[38;5;28mstr\u001b[39m:\n\u001b[0;32m---> 20\u001b[0m     words \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getTopWords\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(words) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m random:\n\u001b[1;32m     22\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mchoice(words)\n",
      "Cell \u001b[0;32mIn[333], line 32\u001b[0m, in \u001b[0;36mModel._getTopWords\u001b[0;34m(self, tokens, n)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(tokens) \u001b[38;5;241m>\u001b[39m req:\n\u001b[1;32m     31\u001b[0m     tokens \u001b[38;5;241m=\u001b[39m tokens[\u001b[38;5;241m-\u001b[39mreq:]\n\u001b[0;32m---> 32\u001b[0m next_words \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mitems(), key\u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m x: x[\u001b[38;5;241m1\u001b[39m], reverse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     33\u001b[0m words \u001b[38;5;241m=\u001b[39m [word[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m next_words \u001b[38;5;28;01mif\u001b[39;00m word[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mthresold]\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwords: \u001b[39m\u001b[38;5;124m\"\u001b[39m,words)\n",
      "\u001b[0;31mKeyError\u001b[0m: ('a', None)"
     ]
    }
   ],
   "source": [
    "sentence = model.complete_sentence(\"I\")\n",
    "print(sentence)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
